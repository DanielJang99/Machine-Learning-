{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML lab 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4.1"
      ],
      "metadata": {
        "id": "sZygmcnaCeYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "df = load_boston()\n",
        "print(df.DESCR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRYXSi0huolC",
        "outputId": "60638275-5cbb-4878-a571-92a2f867bb12"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _boston_dataset:\n",
            "\n",
            "Boston house prices dataset\n",
            "---------------------------\n",
            "\n",
            "**Data Set Characteristics:**  \n",
            "\n",
            "    :Number of Instances: 506 \n",
            "\n",
            "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
            "\n",
            "    :Attribute Information (in order):\n",
            "        - CRIM     per capita crime rate by town\n",
            "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
            "        - INDUS    proportion of non-retail business acres per town\n",
            "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
            "        - NOX      nitric oxides concentration (parts per 10 million)\n",
            "        - RM       average number of rooms per dwelling\n",
            "        - AGE      proportion of owner-occupied units built prior to 1940\n",
            "        - DIS      weighted distances to five Boston employment centres\n",
            "        - RAD      index of accessibility to radial highways\n",
            "        - TAX      full-value property-tax rate per $10,000\n",
            "        - PTRATIO  pupil-teacher ratio by town\n",
            "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
            "        - LSTAT    % lower status of the population\n",
            "        - MEDV     Median value of owner-occupied homes in $1000's\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
            "\n",
            "This is a copy of UCI ML housing dataset.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
            "\n",
            "\n",
            "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
            "\n",
            "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
            "prices and the demand for clean air', J. Environ. Economics & Management,\n",
            "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
            "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
            "pages 244-261 of the latter.\n",
            "\n",
            "The Boston house-price data has been used in many machine learning papers that address regression\n",
            "problems.   \n",
            "     \n",
            ".. topic:: References\n",
            "\n",
            "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
            "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "X = StandardScaler().fit_transform(df.data)\n",
        "y = df.target"
      ],
      "metadata": {
        "id": "xNTicmEfuswc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "def train_test_split(features, target, test_size):\n",
        "  test_index = math.floor(len(features) * test_size)\n",
        "  return features[test_index:], features[:test_index], target[test_index:], target[:test_index]\n",
        "\n",
        "\n",
        "\n",
        "def doOlsReg(features, target, test_size):\n",
        "  print(\"Test Size: \", test_size * 100,\"%\" )\n",
        "  lr = LinearRegression()\n",
        "  train_X, test_X, train_y, test_y = train_test_split(features, target, test_size)\n",
        "\n",
        "  # train the model with test data \n",
        "  lr.fit(train_X, train_y)\n",
        "\n",
        "  # predict y using the model \n",
        "  train_pred = lr.predict(train_X)\n",
        "  test_pred = lr.predict(test_X)\n",
        "\n",
        "  # evaluate mean squared error of the predictions\n",
        "  training_error = mean_squared_error(train_y, train_pred)\n",
        "  test_error = mean_squared_error(test_y, test_pred)\n",
        "  gap = test_error-training_error\n",
        "  print(\"Training Error: \", training_error)\n",
        "  print(\"Test Error: \", test_error)\n",
        "  print(\"Gap (test - training): \", gap, \"\\n\")\n",
        "\n",
        "  return training_error, test_error, gap\n",
        "\n",
        "\n",
        "\n",
        "def olsReport(features, target, test_size_arr):\n",
        "  training_err_list = list()\n",
        "  test_error_list = list()\n",
        "  gap_list = list()\n",
        "\n",
        "  for t_size in test_size_arr:\n",
        "    if t_size >= 1 or t_size < 0:\n",
        "      print(\"test size must be between 0 and 1\")\n",
        "    training_error, test_error, gap = doOlsReg(features, target, t_size)\n",
        "    training_err_list.append(training_error)\n",
        "    test_error_list.append(test_error)\n",
        "    gap_list.append(gap)\n",
        "  \n",
        "  print(\"\\n-----------------------------------------------------------------\\n\")\n",
        "\n",
        "  training_error_mean = np.mean(np.array(training_err_list))\n",
        "  training_error_std = np.std(np.array(training_err_list))\n",
        "  print(\"Training error mean: \", training_error_mean)\n",
        "  print(\"Training error std: \", training_error_std)\n",
        "\n",
        "  test_error_mean = np.mean(np.array(test_error_list))\n",
        "  test_error_std = np.std(np.array(test_error_list))\n",
        "  print(\"Test error mean: \", test_error_mean)\n",
        "  print(\"Test error std: \", test_error_std)\n",
        "\n",
        "  gap_mean = np.mean(np.array(gap_list))\n",
        "  gap_std = np.std(np.array(gap_list))\n",
        "  print(\"Gap mean: \", gap_mean)\n",
        "  print(\"Gap std: \", gap_std)\n",
        "\n",
        "\n",
        "  \n",
        "test_size_list = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "olsReport(X, y, test_size_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sVItOWPOzZB",
        "outputId": "59bcb425-d323-4b5d-a97f-94c20906079b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Size:  10.0 %\n",
            "Training Error:  23.315266235054526\n",
            "Test Error:  9.469518133367723\n",
            "Gap (test - training):  -13.845748101686803 \n",
            "\n",
            "Test Size:  20.0 %\n",
            "Training Error:  24.530576738504493\n",
            "Test Error:  12.57469145018667\n",
            "Gap (test - training):  -11.955885288317823 \n",
            "\n",
            "Test Size:  30.0 %\n",
            "Training Error:  26.41554511773117\n",
            "Test Error:  14.299758130229407\n",
            "Gap (test - training):  -12.115786987501764 \n",
            "\n",
            "Test Size:  40.0 %\n",
            "Training Error:  25.639020939165697\n",
            "Test Error:  20.682354804452437\n",
            "Gap (test - training):  -4.9566661347132595 \n",
            "\n",
            "Test Size:  50.0 %\n",
            "Training Error:  24.281126032685407\n",
            "Test Error:  27.218693490786404\n",
            "Gap (test - training):  2.9375674581009967 \n",
            "\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "Training error mean:  24.836307012628257\n",
            "Training error std:  1.0819985774957739\n",
            "Test error mean:  16.84900320180453\n",
            "Test error std:  6.347444406360384\n",
            "Gap mean:  -7.98730381082373\n",
            "Gap std:  6.2554449683328865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4.2)"
      ],
      "metadata": {
        "id": "9zbYH1fUHYvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_X = list()\n",
        "\n",
        "for i in range(len(X)):\n",
        "  m_form = X[i].reshape(13, 1)\n",
        "  m_form_t = np.transpose(m_form)\n",
        "  \n",
        "  # to get the pair products, turn each value of X into a 13*1 vector, multiply by its transpose, and then get the unique values of the product matrix.\n",
        "  pair_products = np.unique(np.matmul(m_form, m_form_t))\n",
        "\n",
        "  # add 1 for homogenization \n",
        "  features_to_add = np.append(pair_products, 1)\n",
        "  new_features = np.append(X[i], features_to_add)\n",
        "  new_X.append(new_features)\n",
        "\n",
        "new_X = np.array(new_X)\n"
      ],
      "metadata": {
        "id": "CZKKSAiM8kxo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "olsReport(new_X, y, test_size_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMK01fwS-e3M",
        "outputId": "d5e037b2-af22-4e05-8796-dfddebf3c84f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Size:  10.0 %\n",
            "Training Error:  10.148952765188438\n",
            "Test Error:  7.850430848094476\n",
            "Gap (test - training):  -2.2985219170939626 \n",
            "\n",
            "Test Size:  20.0 %\n",
            "Training Error:  10.453906221808344\n",
            "Test Error:  10.315960563407492\n",
            "Gap (test - training):  -0.13794565840085227 \n",
            "\n",
            "Test Size:  30.0 %\n",
            "Training Error:  10.766292481521926\n",
            "Test Error:  13.746629207361462\n",
            "Gap (test - training):  2.9803367258395355 \n",
            "\n",
            "Test Size:  40.0 %\n",
            "Training Error:  9.013353357083744\n",
            "Test Error:  21.827489426502364\n",
            "Gap (test - training):  12.81413606941862 \n",
            "\n",
            "Test Size:  50.0 %\n",
            "Training Error:  8.190650393790754\n",
            "Test Error:  30.674507930822543\n",
            "Gap (test - training):  22.48385753703179 \n",
            "\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "Training error mean:  9.714631043878642\n",
            "Training error std:  0.9649313282891316\n",
            "Test error mean:  16.883003595237668\n",
            "Test error std:  8.35707844751146\n",
            "Gap mean:  7.168372551359026\n",
            "Gap std:  9.24102534119618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With more features and thus higher model complexity, the test error is much higher than the training error, especially when the test size gets higher. Such a stark contrast indicates a possible overfitting of the model. Another evidence of model overfitting comes from the fact that training error mean has decreased signficantly at higher complexity, while we did not observe a dramatic change in test error mean. \n",
        "\n",
        "When test size is small (<= 20%), the difference between training and test error is marginal. However, this may be an indication that the test error is not a good representation of the generalization error due to the small sample size. "
      ],
      "metadata": {
        "id": "0_BVNa8pAnK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4.3)"
      ],
      "metadata": {
        "id": "xXw85bmqHs8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "def ridgeReg(features, target, alpha, test_size=0.4):\n",
        "  print(\"a = \", alpha)\n",
        "  train_X, test_X, train_y, test_y = train_test_split(features, target, test_size)\n",
        "\n",
        "  # train the model with test data \n",
        "  clf = Ridge(alpha=alpha)\n",
        "  clf.fit(train_X, train_y)\n",
        "\n",
        "  # predict y using the model \n",
        "  train_pred = clf.predict(train_X)\n",
        "  test_pred = clf.predict(test_X)\n",
        "\n",
        "  # evaluate mean squared error of the predictions\n",
        "  training_error = mean_squared_error(train_y, train_pred)\n",
        "  test_error = mean_squared_error(test_y, test_pred)\n",
        "  gap = test_error-training_error\n",
        "  print(\"Training Error: \", training_error)\n",
        "  print(\"Test Error: \", test_error)\n",
        "  print(\"Gap (test - training): \", gap, \"\\n\")\n",
        "\n",
        "  return training_error, test_error, gap\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ridgeReport(features, target, alpha_array):\n",
        "  training_err_list = list()\n",
        "  test_error_list = list()\n",
        "  gap_list = list()\n",
        "\n",
        "  for a in alpha_array:\n",
        "    training_error, test_error, gap = ridgeReg(features, target, a)\n",
        "    training_err_list.append(training_error)\n",
        "    test_error_list.append(test_error)\n",
        "    gap_list.append(gap)\n",
        "  \n",
        "  print(\"\\n-----------------------------------------------------------------\\n\")\n",
        "\n",
        "  training_error_mean = np.mean(np.array(training_err_list))\n",
        "  training_error_std = np.std(np.array(training_err_list))\n",
        "  print(\"Training error mean: \", training_error_mean)\n",
        "  print(\"Training error std: \", training_error_std)\n",
        "\n",
        "  test_error_mean = np.mean(np.array(test_error_list))\n",
        "  test_error_std = np.std(np.array(test_error_list))\n",
        "  print(\"Test error mean: \", test_error_mean)\n",
        "  print(\"Test error std: \", test_error_std)\n",
        "\n",
        "  gap_mean = np.mean(np.array(gap_list))\n",
        "  gap_std = np.std(np.array(gap_list))\n",
        "  print(\"Gap mean: \", gap_mean)\n",
        "  print(\"Gap std: \", gap_std)\n",
        "\n",
        "\n",
        "a_array = [0.001, 0.01, 0.1, 1, 10, 100]\n"
      ],
      "metadata": {
        "id": "LNj_3hGwHxg5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ridgeReport(X, y, a_array)\n",
        "ridgeReport(new_X, y, a_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQDJSCkRKdAH",
        "outputId": "1b9d31e4-2518-4cae-b1c0-64e9bf155bab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a =  0.001\n",
            "Training Error:  9.025383865480611\n",
            "Test Error:  21.078099373856627\n",
            "Gap (test - training):  12.052715508376016 \n",
            "\n",
            "a =  0.01\n",
            "Training Error:  9.441470222915367\n",
            "Test Error:  17.837003358814805\n",
            "Gap (test - training):  8.395533135899438 \n",
            "\n",
            "a =  0.1\n",
            "Training Error:  11.62515588042973\n",
            "Test Error:  13.720022997045039\n",
            "Gap (test - training):  2.0948671166153083 \n",
            "\n",
            "a =  1\n",
            "Training Error:  14.052504527173776\n",
            "Test Error:  12.182344472467046\n",
            "Gap (test - training):  -1.87016005470673 \n",
            "\n",
            "a =  10\n",
            "Training Error:  16.050578010642738\n",
            "Test Error:  11.74245270770172\n",
            "Gap (test - training):  -4.3081253029410185 \n",
            "\n",
            "a =  100\n",
            "Training Error:  23.547820776366326\n",
            "Test Error:  17.071458172527187\n",
            "Gap (test - training):  -6.476362603839139 \n",
            "\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "Training error mean:  13.957152213834759\n",
            "Training error std:  4.941733449082545\n",
            "Test error mean:  15.605230180402069\n",
            "Test error std:  3.3484856216605787\n",
            "Gap mean:  1.6480779665673122\n",
            "Gap std:  6.681381405658424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression is often used to penalize coefficients of large magnitude. For the expanded set of features and test size configured to 0.4, the best value of the hyperparameter *a* was =1, which effectively minized the gap. "
      ],
      "metadata": {
        "id": "Ka3ZB1-4LBJt"
      }
    }
  ]
}